# Silveræ ‡å‡†æ ‡æ³¨ - LlamaIndex + Geminiæ–¹æ¡ˆ

## ğŸ“‹ æ€»ä½“æµç¨‹

```
Step 1: æ„å»ºç´¢å¼• (build_llamaindex.py)
  â”œâ”€ è¯»å– corpus.jsonl (92,703ç¯‡è®ºæ–‡)
  â”œâ”€ ä½¿ç”¨ BGE-M3 ç”Ÿæˆ embeddings
  â””â”€ ä¿å­˜ LlamaIndex å‘é‡ç´¢å¼•

Step 2: é“¶æ ‡å‡†æ ‡æ³¨ (label_with_llamaindex.py)
  â”œâ”€ åŠ è½½ç´¢å¼•å’Œ800å¯¹è¯æ•°æ®
  â”œâ”€ å¯¹æ¯ä¸ªturnä½¿ç”¨LlamaIndexæ£€ç´¢top-10
  â”œâ”€ ç”¨Geminiåˆ¤æ–­relevance (0/1/2)
  â””â”€ ä¿å­˜æ ‡æ³¨ç»“æœ

Step 3: è´¨é‡æ£€æŸ¥ (check_labels.py)
  â””â”€ ç»Ÿè®¡åˆ†ææ ‡æ³¨ç»“æœ
```

---

## ğŸ”§ ç¯å¢ƒé…ç½®

### 1. å®‰è£…LlamaIndex

```bash
# å®‰è£…LlamaIndexæ ¸å¿ƒåŒ…
pip install llama-index-core

# å®‰è£…HuggingFace embeddingsæ”¯æŒ
pip install llama-index-embeddings-huggingface

# å¦‚æœéœ€è¦,å®‰è£…å…¶ä»–ç»„ä»¶
pip install llama-index-llms-gemini  # Geminié›†æˆ(å¯é€‰)
```

### 2. ä¾èµ–æ£€æŸ¥

```bash
# å·²æœ‰çš„åŒ… (åº”è¯¥å·²å®‰è£…)
- sentence-transformers  # BGE-M3éœ€è¦
- transformers
- torch
- tqdm
```

### 3. APIé…ç½®

```bash
# Gemini API Key (æ ‡æ³¨æ—¶éœ€è¦)
export GOOGLE_API_KEY="your-gemini-api-key"
```

---

## ğŸ“‚ æ–‡ä»¶ç»“æ„

```
/workspace/PerMed/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ MedCorpus/
â”‚   â”‚   â””â”€â”€ corpus.jsonl              # 92,703ç¯‡è®ºæ–‡
â”‚   â”œâ”€â”€ selected_800_topics.json      # 800ä¸ªé€‰ä¸­çš„topic
â”‚   â””â”€â”€ final_800_topics.jsonl        # 800å¯¹è¯ (3-6è½®)
â”œâ”€â”€ create/
â”‚   â”œâ”€â”€ build_llamaindex.py           # [æ–°] æ„å»ºç´¢å¼•
â”‚   â”œâ”€â”€ label_with_llamaindex.py      # [æ–°] é“¶æ ‡å‡†æ ‡æ³¨
â”‚   â”œâ”€â”€ check_labels.py               # [æ–°] è´¨é‡æ£€æŸ¥
â”‚   â””â”€â”€ llamaindex_index/             # [æ–°] ç´¢å¼•å­˜å‚¨ç›®å½•
â”‚       â”œâ”€â”€ docstore.json
â”‚       â”œâ”€â”€ index_store.json
â”‚       â””â”€â”€ vector_store.json
â””â”€â”€ results/
    â””â”€â”€ silver_labels/                # [æ–°] æ ‡æ³¨ç»“æœ
        â”œâ”€â”€ labels_full.jsonl         # å®Œæ•´æ ‡æ³¨ç»“æœ
        â”œâ”€â”€ labels_top10.jsonl        # Top-10æ ‡æ³¨
        â””â”€â”€ statistics.json           # ç»Ÿè®¡ä¿¡æ¯
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### corpus.jsonl æ ¼å¼
```json
{
  "text_id": "permed-0001",
  "title": "å¯å……æ°”çƒå›Šå‹è¿«è¾…åŠ©ä¸‹...",
  "text": "ç›®çš„æ¢è®¨å¯å……æ°”çƒå›Šå‹è¿«è¾…åŠ©ä¸‹..."
}
```

### ç´¢å¼•é…ç½®
- **åµŒå…¥æ¨¡å‹**: BAAI/bge-m3 (SOTAå¼€æºæ¨¡å‹)
- **è®¾å¤‡**: CUDA (GPUåŠ é€Ÿ)
- **æ‰¹é‡å¤§å°**: 32 (embedding batch)
- **åˆ†å—å¤§å°**: 512 tokens
- **æ–‡æ¡£æ•°**: 92,703ç¯‡

### æ£€ç´¢é…ç½®
- **Top-K**: 10 (æ¯ä¸ªé—®é¢˜æ£€ç´¢top-10æ–‡æ¡£)
- **ç›¸ä¼¼åº¦**: Cosine similarity
- **æ¨¡å¼**: condense_plus_context (è‡ªåŠ¨å¤„ç†å¯¹è¯å†å²)

### æ ‡æ³¨é…ç½®
- **LLM**: Gemini-2.5-Pro
- **Temperature**: 0.0 (ç¡®ä¿ä¸€è‡´æ€§)
- **Relevance**: 0/1/2 (Not/Partial/Highly Relevant)

---

## ğŸš€ è¿è¡Œæ­¥éª¤

### Step 1: æ„å»ºç´¢å¼• (çº¦30-60åˆ†é’Ÿ)

```bash
cd /workspace/PerMed/create

# è¿è¡Œç´¢å¼•æ„å»º
python build_llamaindex.py

# è¾“å‡º:
# [æ­¥éª¤1] åŠ è½½è¯­æ–™åº“... 92,703ç¯‡è®ºæ–‡
# [æ­¥éª¤2] åˆå§‹åŒ–BGE-M3åµŒå…¥æ¨¡å‹...
# [æ­¥éª¤3] æ„å»ºå‘é‡ç´¢å¼•... (30-60åˆ†é’Ÿ)
# [æ­¥éª¤4] ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜...
# [æ­¥éª¤5] éªŒè¯ç´¢å¼•...
# âœ… ç´¢å¼•æ„å»ºå®Œæˆ!
```

**é¢„æœŸè¾“å‡º**:
- `llamaindex_index/docstore.json` - æ–‡æ¡£å­˜å‚¨
- `llamaindex_index/index_store.json` - ç´¢å¼•å…ƒæ•°æ®
- `llamaindex_index/vector_store.json` - å‘é‡æ•°æ®

**èµ„æºéœ€æ±‚**:
- GPUæ˜¾å­˜: ~8-12GB (BGE-M3æ¨¡å‹)
- ç£ç›˜ç©ºé—´: ~5-10GB (ç´¢å¼•æ–‡ä»¶)
- æ—¶é—´: 30-60åˆ†é’Ÿ (å–å†³äºGPU)

### Step 2: é“¶æ ‡å‡†æ ‡æ³¨ (å¾…åˆ›å»º)

```bash
# è¿è¡Œæ ‡æ³¨è„šæœ¬ (ç¨ååˆ›å»º)
python label_with_llamaindex.py

# è¾“å‡º:
# [æ­¥éª¤1] åŠ è½½ç´¢å¼•...
# [æ­¥éª¤2] åŠ è½½800å¯¹è¯...
# [æ­¥éª¤3] æ ‡æ³¨è¿›åº¦: [====>] 800/800 å¯¹è¯
# [æ­¥éª¤4] ä¿å­˜æ ‡æ³¨ç»“æœ...
# âœ… æ ‡æ³¨å®Œæˆ! 36,000 labels
```

### Step 3: è´¨é‡æ£€æŸ¥ (å¾…åˆ›å»º)

```bash
# æ£€æŸ¥æ ‡æ³¨è´¨é‡
python check_labels.py

# è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
```

---

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### LlamaIndex vs ç›´æ¥ä½¿ç”¨BGE-M3

**ä¸ºä»€ä¹ˆç”¨LlamaIndex?**

1. âœ… **å¤šè½®å¯¹è¯ç®¡ç†**: ChatMemoryBufferè‡ªåŠ¨å¤„ç†å†å²
2. âœ… **ç…§åº”è§£æ**: condense_plus_contextæ¨¡å¼è‡ªåŠ¨è§£å†³"å®ƒ"ã€"è¿™ä¸ª"
3. âœ… **ç®€åŒ–ä»£ç **: å‡ è¡Œä»£ç å®ç°å¤æ‚RAG
4. âœ… **å¯å¤ç°æ€§**: 40K+ stars,æ–‡æ¡£é½å…¨

**LlamaIndexåº•å±‚ä½¿ç”¨BGE-M3**:
```python
# LlamaIndexé…ç½®ä½¿ç”¨BGE-M3
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-m3")

# LlamaIndexä¼š:
# 1. ç”¨BGE-M3å¯¹æ–‡æ¡£ç¼–ç 
# 2. å­˜å‚¨å‘é‡åˆ°VectorStore
# 3. æŸ¥è¯¢æ—¶ç”¨BGE-M3å¯¹queryç¼–ç 
# 4. è®¡ç®—cosineç›¸ä¼¼åº¦
# 5. è¿”å›top-kç»“æœ
```

æ‰€ä»¥**æœ¬è´¨ä¸Šè¿˜æ˜¯ç”¨BGE-M3æ£€ç´¢,åªæ˜¯LlamaIndexæä¾›äº†å¤šè½®å¯¹è¯çš„å°è£…**ã€‚

---

## ğŸ“Š é¢„æœŸç»“æœ

### ç´¢å¼•ç»Ÿè®¡
- **æ–‡æ¡£æ•°**: 92,703
- **å¹³å‡é•¿åº¦**: ~500 tokens/doc
- **æ€»å‘é‡æ•°**: ~92,703 (æ¯æ–‡æ¡£1ä¸ªå‘é‡)
- **ç´¢å¼•å¤§å°**: ~5-10GB

### æ ‡æ³¨ç»Ÿè®¡ (é¢„æœŸ)
- **å¯¹è¯æ•°**: 800
- **é—®é¢˜æ•°**: ~3,600 (å¹³å‡4.5 turns/å¯¹è¯)
- **æ ‡æ³¨æ•°**: ~36,000 (æ¯é—®é¢˜top-10æ–‡æ¡£)
- **æ—¶é—´**: ~10å°æ—¶
- **æˆæœ¬**: ~$100 (Gemini API)

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. GPUå†…å­˜

BGE-M3éœ€è¦~8GB GPUæ˜¾å­˜:
```bash
# æ£€æŸ¥GPU
nvidia-smi

# å¦‚æœæ˜¾å­˜ä¸è¶³,å¯ä»¥å‡å°batch_size
embed_batch_size=16  # é»˜è®¤32,å¯é™ä½
```

### 2. ç´¢å¼•æ—¶é—´

92,703ç¯‡æ–‡æ¡£éœ€è¦30-60åˆ†é’Ÿ:
```bash
# å»ºè®®ä½¿ç”¨nohupåå°è¿è¡Œ
nohup python build_llamaindex.py > build_index.log 2>&1 &

# ç›‘æ§è¿›åº¦
tail -f build_index.log
```

### 3. ç´¢å¼•æŒä¹…åŒ–

**é‡è¦**: ç´¢å¼•æ„å»ºå®Œæˆåä¼šä¿å­˜åˆ°ç£ç›˜,ä¹‹åå¯ä»¥ç›´æ¥åŠ è½½,ä¸éœ€è¦é‡æ–°æ„å»º:

```python
# åç»­ä½¿ç”¨æ—¶,ç›´æ¥åŠ è½½å³å¯
from llama_index.core import load_index_from_storage, StorageContext

storage_context = StorageContext.from_defaults(
    persist_dir="/workspace/PerMed/create/llamaindex_index"
)
index = load_index_from_storage(storage_context)
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥

1. âœ… è¿è¡Œ `build_llamaindex.py` æ„å»ºç´¢å¼•
2. â¸ï¸ ç­‰å¾…ç´¢å¼•æ„å»ºå®Œæˆ (30-60åˆ†é’Ÿ)
3. â¸ï¸ åˆ›å»º `label_with_llamaindex.py` æ ‡æ³¨è„šæœ¬
4. â¸ï¸ è¿è¡Œé“¶æ ‡å‡†æ ‡æ³¨
5. â¸ï¸ è´¨é‡æ£€æŸ¥å’Œç»Ÿè®¡

---

## ğŸ“š å‚è€ƒèµ„æ–™

- LlamaIndexæ–‡æ¡£: https://docs.llamaindex.ai/
- BGE-M3: https://huggingface.co/BAAI/bge-m3
- ChatEngine: https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/
- Gemini API: https://ai.google.dev/
