#!/usr/bin/env python3
"""
æ­£ç¡®å®ç°PBR (Personalize Before Retrieve) - ACL 2025

å®Œæ•´åŒ…å«ï¼š
1. P-PRF: LLMç”Ÿæˆä¸ªæ€§åŒ–æŸ¥è¯¢æ‰©å±•ï¼ˆå¼‚æ­¥æ‰¹é‡è°ƒç”¨ï¼‰
2. P-Anchor: åŸºäºå›¾PageRankçš„è®°å¿†æ£€ç´¢
3. ç»„åˆæ£€ç´¢: ä½¿ç”¨æ‰©å±•æŸ¥è¯¢+å›¾ä¸­å¿ƒ+reasoning
"""

import json
import numpy as np
import faiss
import os
import sys
import requests
import asyncio
import aiohttp
from pathlib import Path
from typing import List, Dict, Any, Tuple
from tqdm import tqdm
import argparse
import time
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer


class PBRRetriever:
    """
    å®Œæ•´çš„PBRå®ç°
    
    å‚è€ƒ: /workspace/PBR-code/src/retrieval/retrieval_PBR.py
    """
    
    def __init__(self,
                 corpus_ids: List[str],
                 corpus_texts: List[str],
                 corpus_embeddings: np.ndarray,
                 retriever_model_path: str,
                 llm_api_key: str = None,
                 device: str = "cuda"):
        
        self.corpus_ids = corpus_ids
        self.corpus_texts = corpus_texts
        self.embeddings = corpus_embeddings
        self.device = device
        
        print(f"ğŸš€ åˆå§‹åŒ–PBR Retriever (ACL 2025)")
        print(f"   æ–‡æ¡£æ•°: {len(corpus_ids)}")
        print(f"   Embeddingç»´åº¦: {corpus_embeddings.shape[1]}")
        
        # 1. åŠ è½½embeddingæ¨¡å‹
        print(f"   åŠ è½½embeddingæ¨¡å‹: {retriever_model_path}")
        self.retriever_model = SentenceTransformer(
            retriever_model_path,
            device=device,
            trust_remote_code=True
        )
        
        # 2. åˆ›å»ºFAISSç´¢å¼•
        print(f"   åˆ›å»ºFAISSç´¢å¼•...")
        self.index = faiss.IndexFlatIP(corpus_embeddings.shape[1])
        self.index.add(corpus_embeddings)
        print(f"   âœ… FAISSç´¢å¼•: {self.index.ntotal} å‘é‡")
        
        # 3. LLMé…ç½®ï¼ˆç”¨äºP-PRFï¼‰
        self.llm_api_key = llm_api_key or ""
        self.llm_api_url = "https://api.siliconflow.cn/v1/chat/completions"
        self.llm_model = "Qwen/Qwen3-14B"
        print(f"   LLM: {self.llm_model} (ç”¨äºP-PRF)")
        
        # PBRå‚æ•°
        self.sim_threshold = 0.75
        self.damping_factor = 0.85
    
    async def _call_llm_async(self, session: aiohttp.ClientSession, prompt: str, max_tokens: int = 2048) -> str:
        """å¼‚æ­¥è°ƒç”¨LLM"""
        headers = {
            "Authorization": f"Bearer {self.llm_api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.llm_model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
            "max_tokens": max_tokens,
            "temperature": 0.7
        }
        
        retry = 0
        while retry < 3:
            try:
                async with session.post(
                    self.llm_api_url,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result["choices"][0]["message"]["content"].strip()
                    else:
                        print(f"   âš ï¸ LLMè°ƒç”¨å¤±è´¥ (status {response.status})")
                        retry += 1
                        await asyncio.sleep(1)
            except Exception as e:
                retry += 1
                await asyncio.sleep(1)
                if retry >= 3:
                    print(f"   âš ï¸ LLMè°ƒç”¨å¤±è´¥: {e}")
        return ""
    
    def _call_llm_sync(self, prompt: str, max_tokens: int = 2048) -> str:
        """åŒæ­¥ä¸²è¡Œè°ƒç”¨LLMï¼ˆé¿å…é™æµï¼‰"""
        headers = {
            "Authorization": f"Bearer {self.llm_api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.llm_model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
            "max_tokens": max_tokens,
            "temperature": 0.7
        }
        
        for retry in range(3):
            try:
                response = requests.post(
                    self.llm_api_url,
                    json=payload,
                    headers=headers,
                    timeout=60
                )
                if response.status_code == 200:
                    return response.json()["choices"][0]["message"]["content"].strip()
                elif response.status_code == 429:
                    wait_time = 5 * (retry + 1)  # é€’å¢ç­‰å¾…æ—¶é—´
                    print(f"   âš ï¸ é™æµä¸­ï¼Œç­‰å¾…{wait_time}ç§’...")
                    time.sleep(wait_time)
                else:
                    print(f"   âš ï¸ APIé”™è¯¯ {response.status_code}")
                    time.sleep(2)
            except Exception as e:
                print(f"   âš ï¸ è¯·æ±‚å¤±è´¥: {e}")
                time.sleep(2)
        
        return ""
    
    async def _batch_call_llm_mini_batch(self, prompts: List[str], max_tokens: int = 2048, 
                                          batch_size: int = 10, desc: str = "") -> List[str]:
        """å°æ‰¹é‡å¹¶è¡Œè°ƒç”¨LLMï¼ˆæ‰¹æ¬¡å†…å¹¶è¡Œï¼Œæ‰¹æ¬¡é—´ä¸²è¡Œï¼Œé¿å…é™æµï¼‰"""
        all_responses = []
        num_batches = (len(prompts) + batch_size - 1) // batch_size
        
        print(f"   {desc}: å…±{len(prompts)}ä¸ªè¯·æ±‚ï¼Œåˆ†{num_batches}æ‰¹ï¼Œæ¯æ‰¹{batch_size}ä¸ªå¹¶å‘")
        
        for i in range(0, len(prompts), batch_size):
            batch = prompts[i:i+batch_size]
            batch_num = i // batch_size + 1
            
            print(f"   æ‰¹æ¬¡ {batch_num}/{num_batches} ({len(batch)}ä¸ªè¯·æ±‚)...", end='', flush=True)
            
            # æ‰¹æ¬¡å†…å¹¶è¡Œè°ƒç”¨
            async with aiohttp.ClientSession() as session:
                tasks = [self._call_llm_async(session, prompt, max_tokens) for prompt in batch]
                responses = await asyncio.gather(*tasks)
            
            all_responses.extend(responses)
            print(f" å®Œæˆ")
            
            # æ‰¹æ¬¡é—´ç­‰å¾…ï¼Œé¿å…é™æµ
            if i + batch_size < len(prompts):
                await asyncio.sleep(3)  # æ‰¹æ¬¡é—´éš”3ç§’
        
        return all_responses
    
    def _build_prompts(self, query: str, history: List[str]) -> Tuple[str, str]:
        """
        æ„å»ºP-PRFçš„ä¸¤ä¸ªpromptsï¼ˆä½†ä¸ç«‹å³è°ƒç”¨ï¼‰
        
        Returns:
            prompt_fake: ç”¨äºç”Ÿæˆ10ä¸ªæŸ¥è¯¢å˜ä½“çš„prompt
            prompt_reason: ç”¨äºç”Ÿæˆreasoningçš„prompt
        """
        history_text = "\n".join(history) if history else "No previous dialogue"
        
        prompt_fake = f"""You are to generate 10 natural candidate utterances for medical literature search, inspired by the dialogue history and the current question.

Context
------------
User dialogue history (for style imitation):  
{history_text}

Current question (to inspire the utterances):  
{query}
------------

Guidelines
1. Generate 10 fluent, natural search queries the user might plausibly say.
2. Do NOT just paraphrase; include variations in medical terminology, specificity, or context.
3. Each query > 25 words.
4. Preserve medical terms and concepts.
5. Return ONLY valid JSON in this format (no comments, no markdown):
   {{
     "candidates": [
       "query variation 1...",
       "query variation 2...",
       ...
     ]
   }}
"""
        
        prompt_reason = f"""Solve the medical literature search question step-by-step, inspired by the dialogue history.

Context
------------
User dialogue history (for context):  
{history_text}

Current question:  
{query}
------------
Output (step-by-step reasoning, 2-3 sentences):
"""
        
        return prompt_fake, prompt_reason
    
    def _parse_fake_queries(self, fake_result: str, query: str, history: List[str]) -> List[str]:
        """è§£æLLMç”Ÿæˆçš„fake queries"""
        fake_queries = []
        try:
            import re
            match = re.search(r'\{.*?\}', fake_result, re.DOTALL)
            if match:
                data = json.loads(match.group(0))
                fake_queries = data.get('candidates', [])[:10]
        except:
            pass
        
        if not fake_queries:
            # Fallback: ä½¿ç”¨å†å²ä½œä¸ºæ‰©å±•
            fake_queries = history[:3] if history else [query]
        
        return fake_queries
    
    def _build_memory_graph(self, history_texts: List[str]) -> np.ndarray:
        """
        P-Anchoræ¨¡å—: æ„å»ºè®°å¿†å›¾å¹¶è®¡ç®—å›¾ä¸­å¿ƒ
        
        åŸºäºPBRåŸå§‹å®ç°çš„_build_memory_graphå’Œ_mem_pagerank
        """
        if not history_texts:
            return None
        
        # ç¼–ç å†å²
        history_embeddings = self.retriever_model.encode(
            history_texts,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        
        n = len(history_embeddings)
        if n == 1:
            return history_embeddings[0]
        
        # æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = np.dot(history_embeddings, history_embeddings.T)
        adjacency = (sim_matrix >= self.sim_threshold).astype(float)
        np.fill_diagonal(adjacency, 0)
        
        # PageRank
        out_degree = adjacency.sum(axis=1)
        out_degree[out_degree == 0] = 1
        
        pi = np.ones(n) / n
        for _ in range(50):
            pi_new = (1 - self.damping_factor) * (adjacency.T @ (pi / out_degree)) + self.damping_factor / n
            if np.abs(pi_new - pi).max() < 1e-6:
                break
            pi = pi_new
        
        pi = pi / pi.sum()
        graph_center = np.dot(pi, history_embeddings)
        
        return graph_center
    
    def retrieve_pbr(self, query: str, history: List[str], 
                     fake_queries: List[str] = None, reasoning: str = None,
                     top_k: int = 10, use_llm: bool = True) -> List[Dict]:
        """
        PBRå®Œæ•´æ£€ç´¢æµç¨‹
        
        Args:
            query: å½“å‰æŸ¥è¯¢
            history: å¯¹è¯å†å²
            fake_queries: é¢„å…ˆç”Ÿæˆçš„æŸ¥è¯¢æ‰©å±•ï¼ˆå¦‚æœNoneï¼Œåˆ™ä¸ä½¿ç”¨P-PRFï¼‰
            reasoning: é¢„å…ˆç”Ÿæˆçš„æ¨ç†ï¼ˆå¦‚æœNoneï¼Œåˆ™ä¸ä½¿ç”¨ï¼‰
            top_k: è¿”å›æ•°é‡
            use_llm: æ˜¯å¦ä½¿ç”¨LLMæ‰©å±•ï¼ˆP-PRFï¼‰
        """
        # Step 1: ç¼–ç åŸå§‹æŸ¥è¯¢
        q_embedding = self.retriever_model.encode(
            query,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        
        # Step 2: æ„å»ºè®°å¿†å›¾ä¸­å¿ƒï¼ˆP-Anchorï¼‰
        g_embedding = self._build_memory_graph(history) if history else np.zeros_like(q_embedding)
        
        if use_llm and fake_queries is not None and reasoning is not None:
            # Step 3: ç¼–ç æ‰©å±•æŸ¥è¯¢ï¼ˆP-PRFï¼‰
            if fake_queries:
                prf_embeddings = self.retriever_model.encode(
                    fake_queries,
                    convert_to_numpy=True,
                    normalize_embeddings=True,
                    show_progress_bar=False
                )
                prf_embd_mean = prf_embeddings.mean(axis=0)
            else:
                prf_embd_mean = np.zeros_like(q_embedding)
            
            # ç¼–ç reasoning
            if reasoning:
                reason_embd = self.retriever_model.encode(
                    reasoning,
                    convert_to_numpy=True,
                    normalize_embeddings=True,
                    show_progress_bar=False
                )
            else:
                reason_embd = np.zeros_like(q_embedding)
            
            # Step 4: PBRæ ¸å¿ƒå…¬å¼ï¼ˆæ¥è‡ªåŸå§‹ä»£ç ï¼‰
            avg_qg = (q_embedding + g_embedding) / 2
            w1 = 1 + cosine_similarity(avg_qg[None,:], prf_embd_mean[None,:])[0,0]
            w2 = 1 + cosine_similarity(avg_qg[None,:], reason_embd[None,:])[0,0]
            
            final_query_embedding = q_embedding + g_embedding + w1 * prf_embd_mean + w2 * reason_embd
        
        else:
            # ç®€åŒ–ç‰ˆï¼šä¸ä½¿ç”¨LLMæ‰©å±•
            final_query_embedding = q_embedding + g_embedding
        
        # å½’ä¸€åŒ–
        norm = np.linalg.norm(final_query_embedding)
        if norm > 1e-12:
            final_query_embedding = final_query_embedding / norm
        
        # Step 5: FAISSæ£€ç´¢
        scores, indices = self.index.search(
            final_query_embedding.reshape(1, -1).astype(np.float32),
            top_k
        )
        
        # Step 6: æ„é€ ç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx != -1 and idx < len(self.corpus_ids):
                results.append({
                    "text_id": self.corpus_ids[idx],
                    "score": float(score),
                    "rank": len(results) + 1
                })
        
        return results


async def run_pbr_mini_batch(dataset: str, use_llm: bool = True):
    """è¿è¡ŒPBR baseline - ä½¿ç”¨å°æ‰¹é‡å¹¶è¡ŒLLMè°ƒç”¨ï¼ˆé¿å…é™æµï¼‰"""
    
    print("="*80)
    print(f"PBR Baseline (ACL 2025) - {'å®Œæ•´ç‰ˆ(P-PRF+P-Anchor) [å°æ‰¹é‡å¹¶è¡Œ]' if use_llm else 'ç®€åŒ–ç‰ˆ(P-Anchor)'}")
    print("="*80)
    
    # è·¯å¾„
    data_dir = Path("/workspace/PerMed/baselines/data") / dataset
    output_dir = Path("/workspace/PerMed/baselines/results") / dataset
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. åŠ è½½corpus
    print(f"\nğŸ“š åŠ è½½corpus...")
    corpus_ids, corpus_texts = [], []
    with open(data_dir / "corpus.jsonl", 'r') as f:
        for line in f:
            doc = json.loads(line.strip())
            corpus_ids.append(doc['text_id'])
            corpus_texts.append(f"{doc['title']}ã€‚{doc['text']}")
    print(f"   âœ… {len(corpus_ids)} ä¸ªæ–‡æ¡£")
    
    # 2. åŠ è½½embeddings
    print(f"   åŠ è½½embeddings...")
    embeddings = np.load(data_dir / "corpus_embeddings_qwen3.npy").astype(np.float32)
    print(f"   âœ… Shape: {embeddings.shape}")
    
    # 3. åˆå§‹åŒ–PBR
    pbr = PBRRetriever(
        corpus_ids=corpus_ids,
        corpus_texts=corpus_texts,
        corpus_embeddings=embeddings,
        retriever_model_path="/workspace/PerMed/model/Qwen3-Embedding-0.6B",
        device="cuda"
    )
    
    # 4. åŠ è½½æŸ¥è¯¢
    print(f"\nğŸ“– åŠ è½½æŸ¥è¯¢...")
    queries = []
    with open(data_dir / "queries.jsonl", 'r') as f:
        for line in f:
            queries.append(json.loads(line.strip()))
    
    # ç»Ÿè®¡å¯¹è¯ç»„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    try:
        conversations = set(q.get('conversation_id', q['query_id']) for q in queries)
        print(f"   âœ… {len(queries)} ä¸ªæŸ¥è¯¢ ({len(conversations)} ç»„å¯¹è¯)")
    except:
        print(f"   âœ… {len(queries)} ä¸ªæŸ¥è¯¢")
    
    # 5. æ‰¹é‡å¼‚æ­¥ç”Ÿæˆï¼ˆå¦‚æœä½¿ç”¨LLMï¼‰
    fake_queries_list = [None] * len(queries)
    reasoning_list = [None] * len(queries)
    
    if use_llm:
        print(f"\nğŸ¤– æ‰¹é‡å¼‚æ­¥ç”Ÿæˆ (P-PRF)...")
        print(f"   æ„å»ºprompts...")
        
        # æ„å»ºæ‰€æœ‰prompts
        fake_prompts = []
        reason_prompts = []
        for query_item in queries:
            query = query_item['query']
            history = query_item['history']
            prompt_fake, prompt_reason = pbr._build_prompts(query, history)
            fake_prompts.append(prompt_fake)
            reason_prompts.append(prompt_reason)
        
        print(f"   å°æ‰¹é‡å¹¶è¡Œè°ƒç”¨LLM...")
        fake_responses = await pbr._batch_call_llm_mini_batch(
            fake_prompts, max_tokens=2048, batch_size=20, desc="ç”Ÿæˆfake queries"
        )
        
        reason_responses = await pbr._batch_call_llm_mini_batch(
            reason_prompts, max_tokens=512, batch_size=20, desc="ç”Ÿæˆreasoning"
        )
        
        print(f"   è§£æç»“æœ...")
        for i, (query_item, fake_res, reason_res) in enumerate(zip(queries, fake_responses, reason_responses)):
            fake_queries_list[i] = pbr._parse_fake_queries(fake_res, query_item['query'], query_item['history'])
            reasoning_list[i] = reason_res if reason_res else query_item['query']
        
        print(f"   âœ… æ‰¹é‡ç”Ÿæˆå®Œæˆï¼")
    
    # 6. è¿è¡ŒPBRæ£€ç´¢
    print(f"\nğŸ” PBRæ£€ç´¢...")
    results = []
    for i, query_item in enumerate(tqdm(queries, desc="PBRæ£€ç´¢")):
        query_id = query_item['query_id']
        query = query_item['query']
        history = query_item['history']
        
        try:
            retrieved = pbr.retrieve_pbr(
                query, 
                history, 
                fake_queries=fake_queries_list[i],
                reasoning=reasoning_list[i],
                top_k=10, 
                use_llm=use_llm
            )
            
            results.append({
                "query_id": query_id,
                "query": query,
                "results": retrieved
            })
        except Exception as e:
            print(f"\n   âš ï¸ {query_id} å¤±è´¥: {e}")
            continue
    
    # 7. ä¿å­˜
    output_file = output_dir / ("pbr_full_results.jsonl" if use_llm else "pbr_simple_results.jsonl")
    print(f"\nğŸ’¾ ä¿å­˜: {output_file}")
    with open(output_file, 'w') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    print(f"   âœ… {len(results)} ä¸ªç»“æœ")
    
    print("\n" + "="*80)
    print("âœ… PBRå®Œæˆï¼")
    print("="*80)


def run_pbr_correct(dataset: str, use_llm: bool = True):
    """è¿è¡ŒPBR baselineçš„å…¥å£"""
    asyncio.run(run_pbr_mini_batch(dataset, use_llm))


def main():
    parser = argparse.ArgumentParser(description='PBR Baseline (Correct Implementation)')
    parser.add_argument('--dataset', type=str, default='MedCorpus',
                       choices=['MedCorpus', 'LitSearch'])
    parser.add_argument('--use_llm', action='store_true',
                       help='ä½¿ç”¨LLMè¿›è¡ŒP-PRFæŸ¥è¯¢æ‰©å±•ï¼ˆå®Œæ•´PBRï¼‰')
    args = parser.parse_args()
    
    run_pbr_correct(args.dataset, args.use_llm)


if __name__ == "__main__":
    main()

