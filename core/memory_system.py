# memory_system.py - å®Œæ•´ä¼˜åŒ–ç‰ˆï¼ˆåŒ…å«å¼•ç”¨è§£æï¼‰
import json
from typing import Dict, List, Any, Optional, Tuple, Set
import re
from datetime import datetime
import numpy as np
from collections import defaultdict
import os

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False

from utils import FeatureExtractorRegistry, MemorySystemRegistry, logger, get_config
from sentence_transformers import SentenceTransformer
import torch
import gc

_shared_embedding_model = None
_embedding_model_device = "cpu"

SEQUENTIAL_MEMORY_TAG = "[SEQUENTIAL_MEMORY]"
WORKING_MEMORY_TAG = "[WORKING_MEMORY]"
LONG_EXPLICIT_TAG = "[LONG_EXPLICIT]"

# memory_system.py - ä¼˜åŒ–åçš„ KeyBERTExtractor
@FeatureExtractorRegistry.register('keybert')
class KeyBERTExtractor:
    _instance = None
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(KeyBERTExtractor, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self, model_name=None, config=None):
        if self._initialized: return
        self.config = config or get_config()
        try:
            from keybert import KeyBERT
            if torch.cuda.is_available(): torch.cuda.empty_cache()
            
            # ä½¿ç”¨ Qwen3 æ¨¡å‹ï¼ˆç»Ÿä¸€embeddingæ¨¡å‹ï¼‰
            keybert_embedder_device = getattr(self.config, 'keybert_embedder_device', 'cpu')
            effective_model_name = model_name or getattr(self.config, 'keybert_model',
                '/workspace/PerMed/model/Qwen3-Embedding-0.6B')
            
            # Qwen3 æ¨¡å‹éœ€è¦ trust_remote_code
            keybert_sentence_model = SentenceTransformer(
                effective_model_name,
                device=keybert_embedder_device,
                trust_remote_code=True
            )
            self.model = KeyBERT(model=keybert_sentence_model)
            self._initialized = True
            logger.info(f"KeyBERT initialized with Qwen3 model on {keybert_embedder_device}.")
        except Exception as e:
            logger.error(f"Error initializing KeyBERT: {e}", exc_info=True)
            self._initialized = False
            raise

    def extract_terms(self, text: str, top_n=10) -> List[Tuple[str, float]]:
        """ä½¿ç”¨ Qwen3 æ¨¡å‹ä¼˜åŒ–çš„å…³é”®è¯æå–"""
        if not self._initialized or not text or len(text.strip()) < 5:
            return []
        
        try:
            # GTE æ¨¡å‹æ€§èƒ½æ›´å¼ºï¼Œå¯ä»¥å¤„ç†æ›´å¤æ‚çš„ n-gram
            keywords = self.model.extract_keywords(
                text,
                keyphrase_ngram_range=(1, 3),  # 1-3 è¯çš„çŸ­è¯­
                stop_words='english',
                use_mmr=True,
                diversity=0.4,  # é™ä½å¤šæ ·æ€§ä»¥è·å¾—æ›´ç›¸å…³çš„ç»“æœ
                top_n=top_n * 4,  # å¤šæå–ä¸€äº›ç”¨äºç²¾ç»†ç­›é€‰
                highlight=False,
                nr_candidates=30,  # å¢åŠ å€™é€‰è¯æ•°é‡
                use_maxsum=True  # ä½¿ç”¨ Max Sum ç­–ç•¥è·å¾—æ›´å¥½çš„è¦†ç›–
            )
            
            # ä½¿ç”¨æ›´æ™ºèƒ½çš„åå¤„ç†
            return self._advanced_post_process(keywords, text, top_n)
            
        except Exception as e:
            logger.error(f"KeyBERT extraction error: {e}", exc_info=True)
            return []

    def _advanced_post_process(self, keywords: List[Tuple[str, float]],
                               original_text: str, top_n: int) -> List[Tuple[str, float]]:
        """é«˜çº§åå¤„ç†ï¼Œå……åˆ†åˆ©ç”¨ GTE æ¨¡å‹çš„èƒ½åŠ›"""
        processed = []
        seen_roots = set()  # ç”¨äºå»é‡ç›¸ä¼¼æ¦‚å¿µ
        
        # é¢„å®šä¹‰çš„ç§‘å­¦é¢†åŸŸå¸¸è§æ¨¡å¼
        scientific_patterns = {
            'methods': ['algorithm', 'method', 'approach', 'technique', 'framework', 'model'],
            'materials': ['material', 'compound', 'alloy', 'composite', 'polymer'],
            'processes': ['process', 'synthesis', 'fabrication', 'optimization', 'analysis']
        }
        
        for phrase, score in keywords:
            phrase = phrase.strip()
            
            # è·³è¿‡ä½è´¨é‡çš„ç»“æœ
            if score < 0.15:  # GTE æ¨¡å‹åˆ†æ•°é€šå¸¸æ›´é«˜
                continue
            
            # æ¸…ç†çŸ­è¯­
            words = phrase.split()
            
            # è¿‡æ»¤è§„åˆ™
            if len(words) == 1:
                # å•è¯å¿…é¡»æ˜¯ï¼šä¸“æœ‰åè¯ã€æŠ€æœ¯æœ¯è¯­æˆ–è¶³å¤Ÿé•¿
                if len(phrase) < 5 and not phrase.isupper() and phrase.lower() not in ['gnn', 'ffr', 'sinr', 'ml']:
                    continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„ç§‘å­¦æœ¯è¯­
            is_scientific = any(
                pattern in phrase.lower()
                for patterns in scientific_patterns.values()
                for pattern in patterns
            )
            
            # å»é™¤å†—ä½™ - æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸ä¼¼æ¦‚å¿µ
            is_redundant = False
            phrase_lower = phrase.lower()
            
            for seen in seen_roots:
                # å¦‚æœæ˜¯å­ä¸²æˆ–çˆ¶ä¸²å…³ç³»
                if seen in phrase_lower or phrase_lower in seen:
                    # ä¿ç•™æ›´å…·ä½“çš„ç‰ˆæœ¬
                    if len(phrase) > len(seen):
                        seen_roots.remove(seen)
                        seen_roots.add(phrase_lower)
                    else:
                        is_redundant = True
                    break
            
            if not is_redundant:
                # æå‡ç§‘å­¦æœ¯è¯­çš„åˆ†æ•°
                if is_scientific:
                    score = min(score * 1.2, 1.0)
                
                processed.append((phrase, score))
                seen_roots.add(phrase_lower)
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›
        processed.sort(key=lambda x: x[1], reverse=True)
        return processed[:top_n]

    def extract_concepts(self, text: str, top_n=10) -> List[Tuple[str, float]]:
        """æå–æ¦‚å¿µï¼Œä¸ extract_terms ç›¸åŒä½†å¯ä»¥æœ‰ä¸åŒçš„å‚æ•°"""
        return self.extract_terms(text, top_n)

class SequentialMemory:
    def __init__(self, capacity: int = 10, feature_extractor=None, config=None, embedding_model=None):
        self.recent_queries = []
        self.capacity = capacity
        self.term_usage = defaultdict(int)
        self.feature_extractor = feature_extractor
        self.config = config if config else get_config()
        self.reference_history = []  # æ–°å¢ï¼šå­˜å‚¨å¼•ç”¨å†å²

    def process_query(self, query: str, user_id: str, clicked_docs: List[Dict] = None) -> Dict[str, Any]:
        """å¤„ç†æŸ¥è¯¢ï¼Œè®°å½•å’Œæå–ç›¸å…³æ¦‚å¿µï¼Œå¹¶è¯†åˆ«å¼•ç”¨å…³ç³»"""
        self._update_memory(query, clicked_docs or [])
        
        # æå–å¼•ç”¨ä¿¡æ¯
        reference_info = self._extract_references(query)
        
        # è§£æå¼•ç”¨ï¼ˆå¦‚æœæœ‰å†å²æŸ¥è¯¢ï¼‰
        resolved_references = self._resolve_references(reference_info)
        
        # æå–ä¸å†å²ç›¸å…³çš„æ¦‚å¿µ
        related_concepts = self._extract_related_concepts(query)
        terminology_result = self._detect_terminology_consistency(query)
        
        return {
            "query": query,
            "related_previous_concepts": related_concepts,
            "sequential_terminology": terminology_result,
            "reference_info": reference_info,  # æ–°å¢
            "resolved_references": resolved_references  # æ–°å¢
        }

    def _update_memory(self, query: str, clicked_docs: List[Dict]) -> None:
        """æ›´æ–°è®°å¿†ï¼Œè®°å½•æŸ¥è¯¢å’Œæå–çš„æœ¯è¯­"""
        self.recent_queries.append({
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "clicked_docs": clicked_docs
        })
        
        if len(self.recent_queries) > self.capacity:
            self.recent_queries.pop(0)
        
        # æ›´æ–°æœ¯è¯­ä½¿ç”¨é¢‘ç‡
        if self.feature_extractor:
            for term, score in self.feature_extractor.extract_terms(query, top_n=8):
                # æ ¹æ®åˆ†æ•°åŠ æƒ
                self.term_usage[term] += score

    def _extract_references(self, query: str) -> Dict[str, Any]:
        """æå–æŸ¥è¯¢ä¸­çš„æŒ‡ä»£è¯å’Œè¿æ¥è¯"""
        references = {
            "pronouns": [],
            "connectors": [],
            "full_references": []  # å®Œæ•´çš„å¼•ç”¨çŸ­è¯­
        }
        
        # æŒ‡ä»£è¯æ¨¡å¼ - æ›´å…¨é¢
        pronoun_patterns = [
            (r'\bthese\s+([\w\s-]+?)(?:\s+(?:that|which|who)|[,.])', 'these'),
            (r'\bthis\s+([\w\s-]+?)(?:\s+(?:that|which|who)|[,.])', 'this'),
            (r'\bthose\s+([\w\s-]+?)(?:\s+(?:that|which|who)|[,.])', 'those'),
            (r'\*these\*\s+([\w\s-]+?)(?:\s+|[,.])', 'these_emphasized'),  # å¤„ç†*these*
        ]
        
        # è¿æ¥è¯æ¨¡å¼
        connector_patterns = [
            r'(Building on (?:this|these)[^,]*)',
            r'(Beyond (?:these|this) [\w\s]+)',
            r'(Following (?:this|these)[^,]*)',
            r'(Based on (?:these|this) [\w\s]+)',
            r'(addressing \*?these\*? [\w\s-]+)',
        ]
        
        # æå–æŒ‡ä»£è¯åŠå…¶ä¸Šä¸‹æ–‡
        for pattern, pronoun_type in pronoun_patterns:
            matches = re.finditer(pattern, query, re.IGNORECASE)
            for match in matches:
                context = match.group(1).strip()
                full_match = match.group(0).strip()
                references["pronouns"].append({
                    "type": pronoun_type,
                    "context": context,
                    "full_phrase": full_match,
                    "position": match.start()
                })
        
        # æå–è¿æ¥è¯
        for pattern in connector_patterns:
            matches = re.finditer(pattern, query, re.IGNORECASE)
            for match in matches:
                references["connectors"].append({
                    "phrase": match.group(1),
                    "position": match.start()
                })
                references["full_references"].append(match.group(1))
        
        return references

    def _resolve_references(self, reference_info: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æå¼•ç”¨ï¼Œæ‰¾åˆ°å…·ä½“æŒ‡ä»£çš„å†…å®¹"""
        resolved = {
            "pronoun_resolutions": [],
            "connector_context": []
        }
        
        if not self.recent_queries:
            return resolved
        
        # è·å–ä¸Šä¸€ä¸ªæŸ¥è¯¢çš„å…³é”®æ¦‚å¿µ
        last_query = self.recent_queries[-1]["query"]
        if self.feature_extractor:
            last_concepts = self.feature_extractor.extract_concepts(last_query, top_n=5)
            
            # è§£ææ¯ä¸ªä»£è¯
            for pronoun_info in reference_info.get("pronouns", []):
                pronoun_type = pronoun_info["type"]
                context = pronoun_info["context"]
                
                # æ ¹æ®ä¸Šä¸‹æ–‡æ‰¾åˆ°æœ€å¯èƒ½çš„æŒ‡ä»£
                best_match = self._find_best_reference_match(context, last_concepts, last_query)
                if best_match:
                    resolved["pronoun_resolutions"].append({
                        "original": pronoun_info["full_phrase"],
                        "resolved": best_match,
                        "confidence": 0.8  # å¯ä»¥åŸºäºç›¸ä¼¼åº¦è®¡ç®—
                    })
        
        # è§£æè¿æ¥è¯çš„ä¸Šä¸‹æ–‡
        for connector in reference_info.get("connectors", []):
            resolved["connector_context"].append({
                "connector": connector["phrase"],
                "previous_context": self._get_previous_context_summary()
            })
        
        return resolved

    def _find_best_reference_match(self, context: str, last_concepts: List[Tuple[str, float]],
                                   last_query: str) -> Optional[str]:
        """æ‰¾åˆ°æœ€åŒ¹é…çš„å¼•ç”¨å†…å®¹"""
        context_words = set(context.lower().split())
        best_match = None
        best_score = 0
        
        # æ£€æŸ¥æ¦‚å¿µåŒ¹é…
        for concept, score in last_concepts:
            concept_words = set(concept.lower().split())
            overlap = len(context_words & concept_words)
            if overlap > best_score:
                best_score = overlap
                best_match = concept
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»å®Œæ•´æŸ¥è¯¢ä¸­æå–ç›¸å…³çŸ­è¯­
        if not best_match and context:
            # åœ¨ä¸Šä¸€ä¸ªæŸ¥è¯¢ä¸­æŸ¥æ‰¾ç›¸å…³çš„åè¯çŸ­è¯­
            pattern = r'\b(\w+\s+)?' + re.escape(context.split()[0]) + r'(\s+\w+)*'
            matches = re.findall(pattern, last_query.lower())
            if matches:
                best_match = ' '.join(matches[0]).strip()
        
        return best_match

    def _get_previous_context_summary(self) -> str:
        """è·å–å‰é¢æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡æ‘˜è¦"""
        if not self.recent_queries:
            return ""
        
        last_query = self.recent_queries[-1]["query"]
        if self.feature_extractor:
            concepts = self.feature_extractor.extract_concepts(last_query, top_n=3)
            return ", ".join([c[0] for c in concepts])
        return last_query[:50] + "..."

    def _extract_related_concepts(self, current_query: str) -> Dict[str, Any]:
        """æå–ä¸å†å²æŸ¥è¯¢ç›¸å…³çš„æ¦‚å¿µï¼Œä¸åšè¿ç»­æ€§åˆ¤æ–­"""
        if not self.recent_queries or not self.feature_extractor:
            return {"previous_concepts": [], "shared_concepts": []}
        
        # è·å–å½“å‰æŸ¥è¯¢çš„æ¦‚å¿µ
        current_concepts = {
            concept.lower(): score
            for concept, score in self.feature_extractor.extract_concepts(current_query, top_n=5)
        }
        
        # ä»æœ€è¿‘çš„æŸ¥è¯¢ä¸­æå–ç›¸å…³æ¦‚å¿µ
        all_previous_concepts = []
        shared_concepts = []
        
        # åˆ†ææœ€è¿‘çš„2-3ä¸ªæŸ¥è¯¢
        for recent in self.recent_queries[-3:]:
            if recent["query"]:
                prev_concepts = self.feature_extractor.extract_concepts(recent["query"], top_n=5)
                for concept, score in prev_concepts:
                    concept_lower = concept.lower()
                    all_previous_concepts.append((concept, score))
                    
                    # å¦‚æœåœ¨å½“å‰æŸ¥è¯¢ä¸­ä¹Ÿå‡ºç°
                    if concept_lower in current_concepts:
                        shared_concepts.append((concept, min(score, current_concepts[concept_lower])))
        
        # å»é‡å¹¶æ’åº
        shared_concepts = list(set(shared_concepts))
        shared_concepts.sort(key=lambda x: x[1], reverse=True)
        
        return {
            "previous_concepts": all_previous_concepts[:5],  # æœ€ç›¸å…³çš„å†å²æ¦‚å¿µ
            "shared_concepts": shared_concepts[:3]  # å…±äº«çš„æ¦‚å¿µ
        }

    def _detect_terminology_consistency(self, current_query: str) -> Dict[str, Any]:
        """æ£€æµ‹æœ¯è¯­çš„ä¸€è‡´æ€§ä½¿ç”¨"""
        if not self.feature_extractor:
            return {"detected": False, "consistent_terms": []}
        
        # è·å–é¢‘ç¹ä½¿ç”¨çš„æœ¯è¯­ï¼ˆæ ¹æ®åŠ æƒåˆ†æ•°ï¼‰
        frequent_terms = [
            {"term": term, "weighted_frequency": freq}
            for term, freq in sorted(self.term_usage.items(), key=lambda x: x[1], reverse=True)
            if freq > 0.5  # åŠ æƒé¢‘ç‡é˜ˆå€¼
        ][:10]
        
        return {
            "detected": bool(frequent_terms),
            "consistent_terms": frequent_terms
        }

class WorkingMemory:
    def __init__(self, concept_limit: int = 20, feature_extractor=None, config=None):
        self.current_session_queries = []
        self.current_session_concepts = defaultdict(lambda: {"count": 0, "query_indices": []})
        self.concept_limit = concept_limit
        self.feature_extractor = feature_extractor
        self.config = config or get_config()
        self.session_evolution = []  # æ–°å¢ï¼šè¿½è¸ªsessionæ¼”è¿›

    def process_query(self, query: str, sequential_memory_results: Dict[str, Any], clicked_docs: List[Dict] = None) -> Dict[str, Any]:
        if not query or not self.feature_extractor: return {"session_focus": None, "current_query_core_concepts": []}
        self._update_session_state(query, clicked_docs or [])
        current_query_concepts_data = self.feature_extractor.extract_concepts(query, top_n=5)
        current_query_core_concepts = [concept for concept, score in current_query_concepts_data if score > 0.1]
        session_focus = self._determine_session_focus()
        return {"session_focus": session_focus, "current_query_core_concepts": current_query_core_concepts}

    def _update_session_state(self, query: str, clicked_docs: List[Dict]) -> None:
        self.current_session_queries.append(query)
        query_idx = len(self.current_session_queries) - 1
        if self.feature_extractor:
            concepts = self.feature_extractor.extract_concepts(query, top_n=10)
            # è®°å½•å½“å‰æŸ¥è¯¢çš„ä¸»è¦æ¦‚å¿µç”¨äºæ¼”è¿›è¿½è¸ª
            if concepts:
                main_concept = concepts[0][0] if concepts[0][1] > 0.3 else None
                if main_concept:
                    self.session_evolution.append(main_concept)
            
            for concept, _ in concepts:
                self.current_session_concepts[concept]["count"] += 1
                if query_idx not in self.current_session_concepts[concept]["query_indices"]:
                    self.current_session_concepts[concept]["query_indices"].append(query_idx)

    def _determine_session_focus(self) -> Optional[str]:
        """æ”¹è¿›çš„session focusç¡®å®šï¼Œä½“ç°ç ”ç©¶æ¼”è¿›"""
        if not self.current_session_concepts: return None
        
        # 1. è·å–å½“å‰æŸ¥è¯¢çš„ä¸»è¦æ¦‚å¿µ
        recent_concepts = []
        if len(self.current_session_queries) > 0:
            last_query = self.current_session_queries[-1]
            if self.feature_extractor:
                last_concepts = self.feature_extractor.extract_concepts(last_query, top_n=2)
                recent_concepts = [c[0] for c in last_concepts if c[1] > 0.3]
        
        # 2. è·å–å†å²é«˜é¢‘æ¦‚å¿µ
        sorted_concepts = sorted(
            self.current_session_concepts.items(),
            key=lambda item: (item[1]["count"], len(item[1]["query_indices"])),
            reverse=True
        )
        
        historical_concepts = []
        for concept, data in sorted_concepts[:3]:
            if data["count"] >= 2:
                historical_concepts.append(concept)
        
        # 3. æ„å»ºæ¼”è¿›å¼çš„focus
        if len(self.session_evolution) > 1 and recent_concepts:
            # æ˜¾ç¤ºæ¼”è¿›è·¯å¾„
            if historical_concepts and recent_concepts[0] != historical_concepts[0]:
                return f"{recent_concepts[0]} (evolved from {historical_concepts[0]})"
            else:
                return recent_concepts[0]
        elif recent_concepts and historical_concepts:
            # ç»„åˆå½“å‰å’Œå†å²æ¦‚å¿µ
            if recent_concepts[0] != historical_concepts[0]:
                return f"{recent_concepts[0]} + {historical_concepts[0]}"
            else:
                return recent_concepts[0]
        elif historical_concepts:
            # å¤šä¸ªå†å²æ¦‚å¿µ
            if len(historical_concepts) > 1:
                return " + ".join(historical_concepts[:2])
            else:
                return historical_concepts[0]
        elif sorted_concepts:
            return sorted_concepts[0][0]
        return None

    def new_session(self):
        self.current_session_queries = []
        self.current_session_concepts.clear()
        self.session_evolution = []
        logger.info("WorkingMemory: New session started.")

class LongTermMemory:
    def __init__(self, feature_extractor=None, vector_file=None, embedding_model_name=None, config=None):
        self.explicit_memory = {"research_topics": defaultdict(float), "methodologies": defaultdict(float)}
        self.implicit_memory = {"academic_background": {}, "technical_familiarity": defaultdict(float), "academic_level": "unknown", "level_confidence": 0.0}
        self.feature_extractor = feature_extractor
        self.config = config if config else get_config()
        self.vectors = {"topics": {}, "methods": {}}
        self.indices = {}
        self.topic_evolution = []  # æ–°å¢ï¼šè¿½è¸ªç ”ç©¶ä¸»é¢˜æ¼”å˜

        global _shared_embedding_model, _embedding_model_device
        try:
            if _shared_embedding_model is None:
                _embedding_model_device = self.config.device if torch.cuda.is_available() and self.config.device and self.config.device != "cpu" else "cpu"
                model_path = embedding_model_name or getattr(self.config, 'sentence_transformer_model', 'all-MiniLM-L6-v2')
                use_trust = "gte" in model_path.lower() or "modelscope" in model_path.lower()
                _shared_embedding_model = SentenceTransformer(model_path, device=_embedding_model_device, trust_remote_code=use_trust)
            self.embedding_model = _shared_embedding_model
        except Exception as e:
            logger.error(f"LTM SBERT model error: {e}", exc_info=True)
            self.embedding_model = None

    def update(self, query: str, working_memory_state: Dict[str, Any], clicked_docs: List[Dict] = None) -> None:
        """ä¼˜åŒ–åçš„é•¿æœŸè®°å¿†æ›´æ–°ï¼Œæ›´æ™ºèƒ½çš„ä¸»é¢˜å’Œæ–¹æ³•è®ºè¯†åˆ«"""
        if not self.feature_extractor: return
        try:
            # æå–æ¦‚å¿µæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡æƒé‡
            concepts = self.feature_extractor.extract_concepts(query, top_n=5)
            
            # è®°å½•ä¸»é¢˜æ¼”å˜
            if concepts and concepts[0][1] > 0.3:
                self.topic_evolution.append(concepts[0][0])
            
            for concept, score in concepts:
                # 1. å¯¹ç ”ç©¶ä¸»é¢˜çš„æ›´æ–°è¦è€ƒè™‘è¡°å‡å’Œé˜ˆå€¼
                if concept in self.explicit_memory["research_topics"]:
                    # å·²å­˜åœ¨çš„æ¦‚å¿µï¼Œå¢å¼ºä½†æœ‰è¡°å‡
                    old_score = self.explicit_memory["research_topics"][concept]
                    self.explicit_memory["research_topics"][concept] = old_score * 0.9 + score
                else:
                    # æ–°æ¦‚å¿µï¼Œéœ€è¦è¾ƒé«˜åˆ†æ•°æ‰åŠ å…¥
                    if score > 0.25:  # æé«˜é˜ˆå€¼
                        self.explicit_memory["research_topics"][concept] = score
            
            # 2. å¤„ç†ç‚¹å‡»çš„æ–‡æ¡£
            if clicked_docs:
                for doc in clicked_docs:
                    content = (doc.get("title", "") + " " + doc.get("text", "")).strip()
                    if not content: continue
                    for concept, score in self.feature_extractor.extract_concepts(content, top_n=3):
                        if score > 0.3:
                            self.explicit_memory["research_topics"][concept] += score * 0.5
            
            # 3. æ”¹è¿›çš„æ–¹æ³•è®ºæå–
            method_indicators = [
                "method", "approach", "algorithm", "technique", "analysis",
                "model", "framework", "system", "architecture", "design",
                "optimization", "learning", "network", "process", "strategy"
            ]
            
            # æ–¹æ³•è®ºçŸ­è¯­æ¨¡å¼
            method_patterns = [
                r"(machine|deep|reinforcement) learning",
                r"neural network",
                r"(optimization|prediction|classification) (method|approach|algorithm)",
                r"(computational|experimental|theoretical) (approach|method)",
                r"graph (neural|convolutional) network"
            ]
            
            for concept, score in concepts:
                concept_lower = concept.lower()
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ–¹æ³•è®ºæ¨¡å¼
                is_methodology = False
                
                # æ£€æŸ¥æŒ‡ç¤ºè¯
                if any(indicator in concept_lower for indicator in method_indicators):
                    is_methodology = True
                
                # æ£€æŸ¥æ¨¡å¼åŒ¹é…
                for pattern in method_patterns:
                    if re.search(pattern, concept_lower):
                        is_methodology = True
                        break
                
                # å¦‚æœæ˜¯æ–¹æ³•è®ºä¸”åˆ†æ•°è¶³å¤Ÿé«˜
                if is_methodology and score > 0.3:
                    self.explicit_memory["methodologies"][concept] += score
                    
        except Exception as e:
            logger.error(f"LTM update error: {e}", exc_info=True)

    def retrieve(self, query: str, working_memory_state: Dict[str, Any]) -> Dict[str, Any]:
        # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡ï¼Œé¿å…è¿”å›å¤ªå¤šä½åˆ†é¡¹
        persistent_research_topics = [
            topic for topic, strength in sorted(
                self.explicit_memory["research_topics"].items(),
                key=lambda x: x[1],
                reverse=True
            ) if strength > 0.1  # åªè¿”å›åˆ†æ•°å¤§äº0.1çš„ä¸»é¢˜
        ][:10]  # æœ€å¤šè¿”å›10ä¸ª
        
        persistent_methodologies = [
            method for method, strength in sorted(
                self.explicit_memory["methodologies"].items(),
                key=lambda x: x[1],
                reverse=True
            ) if strength > 0.1
        ][:5]  # æœ€å¤šè¿”å›5ä¸ªæ–¹æ³•
        
        query_relevant_ltm_topics = []
        if self.feature_extractor:
            current_query_concepts = [c for c, _ in self.feature_extractor.extract_concepts(query, top_n=3)]
            for topic in persistent_research_topics:
                # æ›´ä¸¥æ ¼çš„ç›¸å…³æ€§åˆ¤æ–­
                if any(
                    qc.lower() in topic.lower() or
                    topic.lower() in qc.lower() or
                    self._concept_similarity(qc, topic) > 0.7
                    for qc in current_query_concepts
                ):
                    query_relevant_ltm_topics.append(topic)

        return {
            "explicit_memory_keywords": {
                "persistent_research_topics": persistent_research_topics,
                "persistent_methodologies": persistent_methodologies,
                "query_relevant_ltm_topics": query_relevant_ltm_topics[:5]  # é™åˆ¶æ•°é‡
            },
            "implicit_memory_snapshot": self._retrieve_implicit_memory_snapshot()
        }

    def _concept_similarity(self, concept1: str, concept2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ¦‚å¿µçš„ç›¸ä¼¼åº¦"""
        # ç®€å•çš„è¯é‡å ç›¸ä¼¼åº¦
        words1 = set(concept1.lower().split())
        words2 = set(concept2.lower().split())
        if not words1 or not words2:
            return 0.0
        return len(words1 & words2) / len(words1 | words2)

    def _retrieve_implicit_memory_snapshot(self) -> Dict[str, Any]:
        max_items = getattr(self.config, 'max_phrases_per_tag', 3)
        acad_bg = [{"discipline": d.replace("_", " "), "confidence": c} for d,c in sorted(self.implicit_memory["academic_background"].items(),key=lambda x:x[1],reverse=True)[:max_items]]
        top_techs = [{"technology": t, "familiarity": f} for t,f in sorted(self.implicit_memory["technical_familiarity"].items(),key=lambda x:x[1],reverse=True)[:max_items]]
        return {"academic_background_profile": acad_bg,
                "academic_level_profile": {"level": self.implicit_memory["academic_level"], "confidence": self.implicit_memory["level_confidence"]},
                "top_technologies_profile": top_techs}
    
    def new_session(self):
        self.topic_evolution = []

class CognitiveMemorySystem:
    def __init__(self, config=None):
        self.user_profiles = {}
        self.config = config if config else get_config()
        self.dataset_type = getattr(self.config, 'dataset_type', 'unknown')
        self.feature_extractor_type = getattr(self.config, 'feature_extractor', 'keybert')
        global _shared_embedding_model
        self.embedding_model_instance = _shared_embedding_model
        try:
            keybert_model_path = getattr(self.config, 'keybert_model', None)
            self.feature_extractor = FeatureExtractorRegistry.get_extractor(self.feature_extractor_type, model_name=keybert_model_path, config=self.config)
        except Exception as e:
            logger.warning(f"CMS: Failed to init {self.feature_extractor_type} extractor: {e}. Using simple.", exc_info=True)
            self.feature_extractor = self._create_simple_extractor()

    def _create_simple_extractor(self):
        class SimpleExtractor:
            def extract_terms(self, text, top_n=10):
                words=re.findall(r'\b[a-zA-Z]{3,}\b',text.lower())
                c=defaultdict(int)
                for w in words: c[w] += 1
                s={'the','and','is','in','to','of','that','for','on','with','an','are'}
                l=len(words)
                return [(w,v/(l or 1)) for w,v in sorted(c.items(),key=lambda x:x[1],reverse=True) if w not in s][:top_n]
            def extract_concepts(self, text, top_n=10): return self.extract_terms(text, top_n)
        return SimpleExtractor()

    def _get_or_initialize_user_memory(self, user_id: str):
        if user_id not in self.user_profiles:
            logger.info(f"CMS: Initializing new memory profile for user_id: {user_id}")
            self.user_profiles[user_id] = {
                "sequential_memory": SequentialMemory(feature_extractor=self.feature_extractor, config=self.config, embedding_model=self.embedding_model_instance),
                "working_memory": WorkingMemory(feature_extractor=self.feature_extractor, config=self.config),
                "long_term_memory": LongTermMemory(feature_extractor=self.feature_extractor, config=self.config),
                "current_topic_id": None
            }
        return self.user_profiles[user_id]

    def process_query(self, query_text: str, user_id: str, clicked_docs: List[Dict] = None, topic_id: Optional[str] = None) -> Dict[str, Any]:
        user_memory = self._get_or_initialize_user_memory(user_id)
        if topic_id and user_memory.get("current_topic_id") != topic_id:
            user_memory["working_memory"].new_session()
            user_memory["current_topic_id"] = topic_id
        seq_mem, work_mem, lt_mem = user_memory["sequential_memory"], user_memory["working_memory"], user_memory["long_term_memory"]
        seq_res_raw = seq_mem.process_query(query_text, user_id, clicked_docs)
        work_state_raw = work_mem.process_query(query_text, seq_res_raw, clicked_docs)
        lt_mem.update(query_text, work_state_raw, clicked_docs)
        ltm_res_raw = lt_mem.retrieve(query_text, work_state_raw)
        return {"sequential_results_raw": seq_res_raw, "working_memory_state_raw": work_state_raw, "long_term_memory_results_raw": ltm_res_raw}

    def get_tagged_features(self, memory_results_raw: Dict[str, Any],
                            active_components: List[str] = None,
                            turn_id: int = 0) -> List[str]:
        """
        ã€V2ä¼˜åŒ–ã€‘åŠ¨æ€ä¼˜å…ˆçº§çš„ç‰¹å¾æ ‡è®°ç”Ÿæˆ
        
        æ ¸å¿ƒæ”¹è¿›ï¼š
        1. æ‰€æœ‰ç‰¹å¾å€™é€‰éƒ½æœ‰ä¼˜å…ˆçº§åˆ†æ•°
        2. å¼•ç”¨ä¿¡æ¯è·å¾—æœ€é«˜ä¼˜å…ˆçº§ï¼ˆ10.0ï¼‰
        3. æ ¹æ® turn_id åŠ¨æ€è°ƒæ•´å„æ¨¡å—ä¼˜å…ˆçº§
        4. ç¡®ä¿é«˜è´¨é‡ç‰¹å¾ä¸ä¼šå› ä¸ºæ¨¡å—é…é¢è¢«æˆªæ–­
        """
        current_active_components = [c.lower() for c in (active_components or getattr(self.config, 'memory_components', []))]
        config = self.config
        max_overall_features = getattr(config, 'max_tagged_features_for_llm', 10)
        max_kws_in_str = getattr(config, 'max_phrases_per_tag', 5)

        # ===== ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰å€™é€‰ç‰¹å¾å¹¶è¯„åˆ† =====
        all_candidate_features = []  # [(feature_string, priority_score, source_module)]
        used_concepts = set()

        # Sequential Memory - å¼•ç”¨ä¿¡æ¯ç»™æœ€é«˜ä¼˜å…ˆçº§
        if 'sequential' in current_active_components:
            seq_raw = memory_results_raw.get("sequential_results_raw", {})
            reference_info = seq_raw.get("reference_info", {})
            resolved_refs = seq_raw.get("resolved_references", {})
            
            # 1. å¼•ç”¨è§£æ - æœ€é«˜ä¼˜å…ˆçº§ï¼ˆ10.0ï¼‰
            if reference_info.get("pronouns") or reference_info.get("connectors"):
                ref_features = []
                
                # å¤„ç†ä»£è¯è§£æ
                for resolution in resolved_refs.get("pronoun_resolutions", []):
                    ref_features.append(f"'{resolution['original']}' refers to: {resolution['resolved']}")
                
                # å¤„ç†è¿æ¥è¯
                for connector_info in resolved_refs.get("connector_context", []):
                    ref_features.append(f"Context for '{connector_info['connector']}': {connector_info['previous_context']}")
                
                if ref_features:
                    feature_str = f"{SEQUENTIAL_MEMORY_TAG} References: {'; '.join(ref_features[:2])}"
                    priority = 10.0  # ğŸ”¥ æœ€é«˜ä¼˜å…ˆçº§
                    all_candidate_features.append((feature_str, priority, "sequential"))
                    logger.debug(f"Found reference with priority {priority}")
            
            # 2. å…±äº«æ¦‚å¿µ - é«˜ä¼˜å…ˆçº§ï¼ˆ7.5-8.5ï¼‰
            related_info = seq_raw.get("related_previous_concepts", {})
            shared_concepts = related_info.get("shared_concepts", [])
            
            if shared_concepts:
                concepts_str = ", ".join([c[0] for c in shared_concepts[:max_kws_in_str] if c[0] not in used_concepts])
                if concepts_str:
                    # ä¼˜å…ˆçº§æ ¹æ®å…±äº«æ¦‚å¿µæ•°é‡åŠ¨æ€è°ƒæ•´
                    priority = 7.5 + min(len(shared_concepts) * 0.2, 1.0)
                    feature_str = f"{SEQUENTIAL_MEMORY_TAG} Continuing exploration of: {concepts_str}"
                    all_candidate_features.append((feature_str, priority, "sequential"))
                    used_concepts.update([c[0] for c in shared_concepts[:max_kws_in_str]])
            
            # 3. æœ¯è¯­ä¸€è‡´æ€§ - ä¸­ç­‰ä¼˜å…ˆçº§ï¼ˆ5.0-6.5ï¼‰
            term_info = seq_raw.get("sequential_terminology", {})
            consistent_terms = [
                item['term'] for item in term_info.get("consistent_terms", [])[:max_kws_in_str]
                if item['weighted_frequency'] > 1.0 and item['term'] not in used_concepts
            ]
            
            if consistent_terms:
                priority = 5.0 + min(len(consistent_terms) * 0.3, 1.5)
                feature_str = f"{SEQUENTIAL_MEMORY_TAG} Established terminology: {', '.join(consistent_terms)}"
                all_candidate_features.append((feature_str, priority, "sequential"))
                used_concepts.update(consistent_terms)

        # Working Memory - æ ¹æ®è½®æ¬¡åŠ¨æ€è°ƒæ•´ä¼˜å…ˆçº§
        if 'working' in current_active_components:
            wm_raw = memory_results_raw.get("working_memory_state_raw", {})
            focus = wm_raw.get("session_focus")
            
            if focus:
                # ğŸ¯ Turn 1-2: é«˜ä¼˜å…ˆçº§ï¼ˆå¸®åŠ©å»ºç«‹ä¸Šä¸‹æ–‡ï¼‰
                # Turn 3+: ä¸­ç­‰ä¼˜å…ˆçº§ï¼ˆä¸Šä¸‹æ–‡å·²å»ºç«‹ï¼‰
                if turn_id <= 2:
                    priority = 9.0  # æ—©æœŸè½®æ¬¡ï¼Œç„¦ç‚¹å¾ˆé‡è¦
                else:
                    priority = 6.0  # åæœŸè½®æ¬¡ï¼Œé™ä½ä¼˜å…ˆçº§
                
                if "evolved from" in focus:
                    feature_str = f"{WORKING_MEMORY_TAG} Research evolution: {focus}"
                    priority += 0.5  # æ¼”è¿›ä¿¡æ¯é¢å¤–åŠ åˆ†
                elif " + " in focus:
                    feature_str = f"{WORKING_MEMORY_TAG} Current session exploring: {focus}"
                else:
                    feature_str = f"{WORKING_MEMORY_TAG} Session focus: {focus}"
                
                all_candidate_features.append((feature_str, priority, "working"))
                
                # è®°å½•focusä¸­çš„æ¦‚å¿µ
                focus_concepts = re.findall(r'\b\w+\b', focus.lower())
                used_concepts.update(focus_concepts)
            
            # å½“å‰æŸ¥è¯¢æ ¸å¿ƒæ¦‚å¿µ - å§‹ç»ˆé‡è¦
            core_concepts = wm_raw.get("current_query_core_concepts", [])
            if core_concepts:
                meaningful_concepts = [
                    c for c in core_concepts[:max_kws_in_str]
                    if len(c) > 3 and c not in used_concepts
                ]
                if meaningful_concepts:
                    priority = 8.5  # å½“å‰æ¦‚å¿µæ€»æ˜¯é‡è¦
                    feature_str = f"{WORKING_MEMORY_TAG} Query emphasizes: {', '.join(meaningful_concepts)}"
                    all_candidate_features.append((feature_str, priority, "working"))
                    used_concepts.update(meaningful_concepts)

        # Long-Term Memory - åæœŸè½®æ¬¡æ‰é‡è¦
        if 'long' in current_active_components:
            ltm_raw = memory_results_raw.get("long_term_memory_results_raw", {})
            explicit_kws = ltm_raw.get("explicit_memory_keywords", {})
            
            # ğŸ¯ LTM çš„ä¼˜å…ˆçº§éšè½®æ¬¡å¢åŠ è€Œæé«˜
            ltm_base_priority = min(3.0 + turn_id * 0.5, 7.0)
            
            # ç ”ç©¶ä¸»é¢˜
            persistent_topics = explicit_kws.get("persistent_research_topics", [])
            if persistent_topics:
                top_topics = [
                    t for t in persistent_topics[:max_kws_in_str]
                    if len(t) > 4 and t not in used_concepts
                ]
                if top_topics:
                    priority = ltm_base_priority
                    feature_str = f"{LONG_EXPLICIT_TAG} Established research areas: {', '.join(top_topics)}"
                    all_candidate_features.append((feature_str, priority, "long"))
                    used_concepts.update(top_topics)
            
            # æ–¹æ³•è®ºä¸“é•¿
            persistent_methods = explicit_kws.get("persistent_methodologies", [])
            if persistent_methods:
                method_list = [m for m in persistent_methods[:max_kws_in_str] if m not in used_concepts]
                if method_list:
                    priority = ltm_base_priority + 0.5
                    feature_str = f"{LONG_EXPLICIT_TAG} Methodological expertise: {', '.join(method_list)}"
                    all_candidate_features.append((feature_str, priority, "long"))
                    used_concepts.update(method_list)
            
            # æŸ¥è¯¢ç›¸å…³çš„å†å²å…´è¶£
            query_relevant_ltm = explicit_kws.get("query_relevant_ltm_topics", [])
            if query_relevant_ltm and turn_id >= 2:  # åªåœ¨ç¬¬2è½®åä½¿ç”¨
                relevant_unique = [
                    t for t in query_relevant_ltm[:max_kws_in_str]
                    if t not in used_concepts
                ]
                if relevant_unique:
                    priority = ltm_base_priority + 0.3
                    feature_str = f"{LONG_EXPLICIT_TAG} Related past interests: {', '.join(relevant_unique)}"
                    all_candidate_features.append((feature_str, priority, "long"))
        
        # ===== ç¬¬äºŒæ­¥ï¼šæŒ‰ä¼˜å…ˆçº§æ’åºå¹¶é€‰æ‹© =====
        all_candidate_features.sort(key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©å‰Nä¸ªï¼Œä½†ç¡®ä¿å¤šæ ·æ€§ï¼ˆä¸è¦å…¨æ¥è‡ªä¸€ä¸ªæ¨¡å—ï¼‰
        selected_features = []
        module_counts = defaultdict(int)
        max_per_module = max(max_overall_features // 2, 3)  # ä»»ä½•æ¨¡å—æœ€å¤šå 50%ï¼Œä½†è‡³å°‘3æ¡
        
        for feature_str, priority, source_module in all_candidate_features:
            if len(selected_features) >= max_overall_features:
                break
            
            # é¿å…æŸä¸ªæ¨¡å—å„æ–­ï¼ˆä½†ä¼˜å…ˆçº§>=9.5çš„æ€»æ˜¯ä¿ç•™ï¼‰
            if priority < 9.5 and module_counts[source_module] >= max_per_module:
                continue
            
            selected_features.append(feature_str)
            module_counts[source_module] += 1
        
        logger.info(f"Turn {turn_id}: Selected {len(selected_features)} features. "
                   f"Distribution: {dict(module_counts)}")
        if all_candidate_features:
            top3 = [(f[:60] + "..." if len(f) > 60 else f, f"{p:.1f}") 
                    for f, p, _ in all_candidate_features[:3]]
            logger.debug(f"Top 3 priorities: {top3}")
        
        return selected_features

    def new_user_session(self, user_id: str, topic_id: str):
        self._get_or_initialize_user_memory(user_id)
        user_memory = self.user_profiles[user_id]
        if user_memory.get("current_topic_id") != topic_id:
            logger.info(f"CMS: New session for user '{user_id}', topic_id '{topic_id}'. Resetting Working Memory.")
            if "working_memory" in user_memory and hasattr(user_memory["working_memory"], "new_session"):
                user_memory["working_memory"].new_session()
            user_memory["current_topic_id"] = topic_id
